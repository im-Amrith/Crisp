{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "  print(f'User uploaded file \"{filename}\" with length {len(uploaded[filename])} bytes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "oB3pPmoq6HaC",
        "outputId": "edaa86b0-ea1e-4b16-ae14-eec5e278dc29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c67fedcb-b80c-4e99-a246-d9a3c191ec08\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c67fedcb-b80c-4e99-a246-d9a3c191ec08\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Aquaponics Water Quality Data.csv to Aquaponics Water Quality Data.csv\n",
            "Saving GE_RefractoryAlloyScreeningDataset_FINAL.csv to GE_RefractoryAlloyScreeningDataset_FINAL.csv\n",
            "User uploaded file \"Aquaponics Water Quality Data.csv\" with length 116738 bytes\n",
            "User uploaded file \"GE_RefractoryAlloyScreeningDataset_FINAL.csv\" with length 4706 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import joblib\n",
        "\n",
        "def clean_col_names(df):\n",
        "    \"\"\"\n",
        "    Cleans column names of a pandas DataFrame by replacing spaces with\n",
        "    underscores and removing special characters.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with cleaned column names.\n",
        "    \"\"\"\n",
        "    cols = df.columns\n",
        "    new_cols = []\n",
        "    for col in cols:\n",
        "        new_col = re.sub(r'[^a-zA-Z0-9_]', '', col.strip().replace(' ', '_'))\n",
        "        new_cols.append(new_col)\n",
        "    df.columns = new_cols\n",
        "    return df\n",
        "\n",
        "def load_aquaponics_data(data_path):\n",
        "    \"\"\"\n",
        "    Loads and performs initial preprocessing on the Aquaponics Water Quality Data.\n",
        "\n",
        "    Args:\n",
        "        data_path (str): The path to the Aquaponics CSV data file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The loaded and initially preprocessed DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the specified data file does not exist.\n",
        "        Exception: For errors during file reading or initial cleaning.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(data_path):\n",
        "        raise FileNotFoundError(f\"Data file not found: {data_path}\")\n",
        "\n",
        "    print(f\"Loading Aquaponics data from {data_path}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(data_path)\n",
        "        df = clean_col_names(df)\n",
        "        print(f\"Successfully loaded {os.path.basename(data_path)} with shape {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error loading {os.path.basename(data_path)}: {e}\")\n",
        "\n",
        "\n",
        "def preprocess_aquaponics_data(df):\n",
        "    \"\"\"\n",
        "    Performs detailed preprocessing on the Aquaponics Water Quality Data DataFrame.\n",
        "\n",
        "    Includes datetime conversion, feature extraction, handling missing values,\n",
        "    dropping irrelevant columns, and identifying feature types.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw Aquaponics DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The preprocessed DataFrame.\n",
        "    \"\"\"\n",
        "    print(\"\\nPreprocessing Aquaponics data...\")\n",
        "    # Convert 'created_at' to datetime and extract time-based features\n",
        "    if 'created_at' in df.columns:\n",
        "        # Using errors='coerce' to handle potential parsing issues\n",
        "        df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
        "        # Drop rows where created_at could not be parsed\n",
        "        df.dropna(subset=['created_at'], inplace=True)\n",
        "        if not df.empty:\n",
        "            df['hour'] = df['created_at'].dt.hour\n",
        "            df['day_of_week'] = df['created_at'].dt.dayofweek\n",
        "            # Added more granular time features\n",
        "            df['month'] = df['created_at'].dt.month\n",
        "            df['day'] = df['created_at'].dt.day\n",
        "            df['year'] = df['created_at'].dt.year\n",
        "            df['weekofyear'] = df['created_at'].dt.isocalendar().week.astype(int)\n",
        "\n",
        "            # Add lag features for relevant numeric columns\n",
        "            # Identify numeric columns excluding time features just added and potential targets\n",
        "            numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "            lag_features_to_consider = [col for col in numeric_cols if col not in ['hour', 'day_of_week', 'month', 'day', 'year', 'weekofyear']]\n",
        "\n",
        "            # Sort by time before creating lag features\n",
        "            df.sort_values(by='created_at', inplace=True)\n",
        "\n",
        "            for col in lag_features_to_consider:\n",
        "                 # Add 1-period lag feature\n",
        "                 df[f'{col}_lag1'] = df[col].shift(1)\n",
        "                 # Add 2-period lag feature (optional, can extend)\n",
        "                 df[f'{col}_lag2'] = df[col].shift(2)\n",
        "\n",
        "\n",
        "        df.drop('created_at', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "    # Drop entry_id if it exists and is not useful as a feature\n",
        "    if 'entry_id' in df.columns:\n",
        "        df.drop('entry_id', axis=1, inplace=True)\n",
        "\n",
        "    # Drop 'Unnamed' columns with mostly NaNs\n",
        "    df = df.loc[:, ~df.columns.str.startswith('Unnamed_')]\n",
        "\n",
        "    # Identify potential categorical columns that are currently objects\n",
        "    # Excluding 'Notes' as it's likely free text and not easily encoded\n",
        "    categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "    categorical_cols = [col for col in categorical_cols if col not in ['Notes', 'Day', 'Date', 'Time']] # Exclude original date/time columns if created_at was used\n",
        "\n",
        "    print(f\"Identified potential categorical columns for encoding: {categorical_cols}\")\n",
        "\n",
        "    # Drop columns that are not features or targets and are not being encoded\n",
        "    all_possible_features_targets = categorical_cols + df.select_dtypes(include=np.number).columns.tolist() + ['Notes'] # Include Notes to explicitly drop later if needed\n",
        "    cols_to_drop = [col for col in df.columns if col not in all_possible_features_targets]\n",
        "    if cols_to_drop:\n",
        "        print(f\"Dropping non-feature/target columns: {cols_to_drop}\")\n",
        "        df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "\n",
        "    print(f\"Aquaponics DataFrame shape after preprocessing: {df.shape}\")\n",
        "    print(\"Aquaponics DataFrame info after preprocessing:\")\n",
        "    df.info()\n",
        "    return df, categorical_cols # Return categorical columns list\n",
        "\n",
        "def train_and_evaluate_model(df, target, categorical_features, models_dir, model_name_prefix):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a Random Forest Regressor model for a specific target variable.\n",
        "\n",
        "    Includes preprocessing pipeline with imputation, scaling, one-hot encoding,\n",
        "    and optional target normalization. Uses GridSearchCV for hyperparameter tuning.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame containing features and the target.\n",
        "        target (str): The name of the target variable column.\n",
        "        categorical_features (list): A list of categorical feature column names.\n",
        "        models_dir (str): Directory to save the trained model.\n",
        "        model_name_prefix (str): Prefix to use for saving the model file (e.g., 'aquaponics').\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing evaluation metrics (MSE, R2) and best parameters,\n",
        "              or None if training/evaluation is skipped or fails.\n",
        "    \"\"\"\n",
        "    print(f\"\\nTraining model for: {target}\")\n",
        "\n",
        "    # Check if the target column exists and has non-null values\n",
        "    if target not in df.columns or df[target].dropna().empty:\n",
        "        print(f\"Skipping {target}: Target column not found or contains no non-null values.\")\n",
        "        return None\n",
        "\n",
        "    # Drop rows where the target variable is NaN\n",
        "    df_target = df.dropna(subset=[target]).copy()\n",
        "\n",
        "    # Dynamically identify features after dropping NaNs in the target\n",
        "    # Exclude the target column and the 'Notes' column\n",
        "    features = [col for col in df_target.columns if col != target and col != 'Notes']\n",
        "\n",
        "    # Separate numeric and categorical features from the *actual* features available in df_target\n",
        "    numeric_features = df_target[features].select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_features = df_target[features].select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "\n",
        "    if not features: # Check if any features remain after exclusions\n",
        "        print(f\"Skipping {target}: No valid features found after dropping NaNs and excluding target/Notes.\")\n",
        "        return None\n",
        "\n",
        "    X = df_target[features]\n",
        "    y = df_target[target]\n",
        "\n",
        "    if X.empty or y.empty:\n",
        "        print(f\"Skipping {target}: X or y is empty after preprocessing or filtering.\")\n",
        "        return None\n",
        "\n",
        "    # Implement Target Normalization (e.g., PowerTransformer)\n",
        "    # Only apply if the target variable has positive values and some variance\n",
        "    target_transformer = None\n",
        "    original_y = y.copy() # Keep original for inverse transform\n",
        "\n",
        "    if y.min() >= 0 and y.var() > 1e-6: # Check for non-negative values and variance\n",
        "        print(f\"Applying PowerTransformer to target: {target}\")\n",
        "        target_transformer = PowerTransformer(method='yeo-johnson') # Yeo-Johnson handles zero/negative\n",
        "        try:\n",
        "             # Need to reshape y for the transformer\n",
        "            y = target_transformer.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "        except ValueError as e:\n",
        "             print(f\"Could not apply PowerTransformer to {target}: {e}. Skipping transformation.\")\n",
        "             y = original_y # Revert to original if transformation fails\n",
        "             target_transformer = None # Don't use transformer if it failed\n",
        "\n",
        "\n",
        "    # Create preprocessing pipelines for numeric and categorical features\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')), # Impute before scaling\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # Impute before encoding\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "\n",
        "    # Create a column transformer to apply different transformations to different columns\n",
        "    # Include both numeric and categorical transformers\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)],\n",
        "        remainder='passthrough') # Keep other columns (if any, though 'drop' was used earlier)\n",
        "\n",
        "\n",
        "    # Define the full pipeline including preprocessing and the regressor\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               ('regressor', RandomForestRegressor(random_state=42, n_jobs=-1))])\n",
        "\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Fit the model with GridSearchCV for hyperparameter tuning\n",
        "    try:\n",
        "        param_grid = {\n",
        "            'regressor__n_estimators': [100, 200, 300], # Test more values\n",
        "            'regressor__max_depth': [None, 10, 20, 30], # Test more depths\n",
        "            'regressor__min_samples_split': [2, 5, 10] # Test min samples split\n",
        "        }\n",
        "        # Using k-fold cross-validation within GridSearchCV\n",
        "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1) # Increased folds to 5\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        best_model = grid_search.best_estimator_ # Get the best model from GridSearchCV\n",
        "        print(f\"Best parameters for {target}: {grid_search.best_params_}\")\n",
        "        print(f\"Best cross-validation R2 score for {target}: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Error fitting pipeline or GridSearchCV for {target}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "         print(f\"An unexpected error occurred during model training for {target}: {e}\")\n",
        "         return None\n",
        "\n",
        "\n",
        "    # Save the model\n",
        "    model_filename = os.path.join(models_dir, f\"{model_name_prefix}_{target}.joblib\")\n",
        "    try:\n",
        "        joblib.dump(best_model, model_filename)\n",
        "        print(f\"Model for {target} saved to {model_filename}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error saving model for {target} to {model_filename}: {e}\")\n",
        "        # Continue evaluation even if saving fails\n",
        "\n",
        "    # Evaluate the best model on the test set\n",
        "    try:\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        # Inverse transform predictions if target was transformed\n",
        "        if target_transformer:\n",
        "            y_pred_original_scale = target_transformer.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
        "            # Also inverse transform the test set target for correct evaluation\n",
        "            y_test_original_scale = target_transformer.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "            mse = mean_squared_error(y_test_original_scale, y_pred_original_scale)\n",
        "            r2 = r2_score(y_test_original_scale, y_pred_original_scale)\n",
        "            print(f\"Evaluation on original scale for {target} (after inverse transform):\")\n",
        "        else:\n",
        "            # If no transformation, evaluate on the scale of y_test\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "        print(f\"Test Set Mean Squared Error for {target}: {mse:.4f}\")\n",
        "        print(f\"Test Set R2 Score for {target}: {r2:.4f}\")\n",
        "        return {\"mse\": mse, \"r2\": r2, \"best_params\": grid_search.best_params_, \"cv_r2\": grid_search.best_score_}\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating model for {target}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Configuration ---\n",
        "    aquaponics_data_filename = \"Aquaponics Water Quality Data.csv\"\n",
        "    data_directory = \"/content\" # Assuming uploaded files are in /content\n",
        "    aquaponics_data_path = os.path.join(data_directory, aquaponics_data_filename)\n",
        "    models_dir = \"/content/trained_models_aquaponics\" # Separate models directory for aquaponics\n",
        "    aquaponics_preprocessed_path = \"/content/preprocessed_aquaponics_data.csv\"\n",
        "\n",
        "    # Create models directory if it doesn't exist\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "    # --- Process Aquaponics Data ---\n",
        "    aquaponics_df = None\n",
        "    try:\n",
        "        aquaponics_df_raw = load_aquaponics_data(aquaponics_data_path)\n",
        "        aquaponics_df, aquaponics_categorical_features = preprocess_aquaponics_data(aquaponics_df_raw)\n",
        "\n",
        "        # Save the preprocessed Aquaponics data\n",
        "        try:\n",
        "            aquaponics_df.to_csv(aquaponics_preprocessed_path, index=False)\n",
        "            print(f\"Preprocessed Aquaponics data saved to {aquaponics_preprocessed_path}\")\n",
        "        except IOError as e:\n",
        "            print(f\"Error saving preprocessed Aquaponics data: {e}\")\n",
        "\n",
        "        # Define target variables for Aquaponics data as requested\n",
        "        aquaponics_target_variables = [\"Ammonia_ppm\", \"pH\", \"Dissolved_Oxygen_ppm\", \"Water_Temp_F\", \"Nitrate_ppm\"]\n",
        "\n",
        "        # Identify ALL potential features before filtering for each target\n",
        "        # Exclude target variables, Notes, and Unnamed columns from the raw features pool\n",
        "        all_potential_features = [col for col in aquaponics_df.columns if col not in aquaponics_target_variables and col != 'Notes' and not col.startswith('Unnamed_')]\n",
        "\n",
        "        # We don't strictly need aquaponics_features list here anymore as features are determined dynamically in the training function\n",
        "\n",
        "\n",
        "        print(\"\\nStarting ML model development for Aquaponics data...\")\n",
        "        aquaponics_evaluation_results = {}\n",
        "        for target_var in aquaponics_target_variables:\n",
        "             # Pass categorical features list to train_and_evaluate_model\n",
        "             results = train_and_evaluate_model(aquaponics_df, target_var, aquaponics_categorical_features, models_dir, \"aquaponics\")\n",
        "             if results:\n",
        "                 aquaponics_evaluation_results[target_var] = results\n",
        "\n",
        "        print(\"\\nML model development for Aquaponics data complete.\")\n",
        "        print(\"\\nAquaponics Evaluation Results:\")\n",
        "        for target, metrics in aquaponics_evaluation_results.items():\n",
        "            print(f\"{target}: Test MSE = {metrics['mse']:.4f}, Test R2 = {metrics['r2']:.4f}, CV R2 = {metrics['cv_r2']:.4f}, Best Params = {metrics['best_params']}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Aquaponics data processing: {e}\")\n",
        "\n",
        "\n",
        "    print(\"\\nOverall process complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS6BIQfV6Fyh",
        "outputId": "20417a9f-f522-401b-bcdc-c5ca89821e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Aquaponics data from /content/Aquaponics Water Quality Data.csv...\n",
            "Successfully loaded Aquaponics Water Quality Data.csv with shape (366, 29)\n",
            "\n",
            "Preprocessing Aquaponics data...\n",
            "Identified potential categorical columns for encoding: ['Recorded_by_Remy_RomoValdez_x__not_recorder_0__recorder', 'Javier_Mollinedo_x__not_recorder_0__recorder', 'Recorded_by_Joe_Tocci_x__not_recorder_0__recorder', 'Recorded_by_Ellison_Montgomery_x__not_recorder_0__recorder']\n",
            "Dropping non-feature/target columns: ['Day', 'Date', 'Time']\n",
            "Aquaponics DataFrame shape after preprocessing: (366, 12)\n",
            "Aquaponics DataFrame info after preprocessing:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 366 entries, 0 to 365\n",
            "Data columns (total 12 columns):\n",
            " #   Column                                                      Non-Null Count  Dtype  \n",
            "---  ------                                                      --------------  -----  \n",
            " 0   Recorded_by_Remy_RomoValdez_x__not_recorder_0__recorder     366 non-null    object \n",
            " 1   Javier_Mollinedo_x__not_recorder_0__recorder                366 non-null    object \n",
            " 2   Recorded_by_Joe_Tocci_x__not_recorder_0__recorder           366 non-null    object \n",
            " 3   Recorded_by_Ellison_Montgomery_x__not_recorder_0__recorder  366 non-null    object \n",
            " 4   pH                                                          366 non-null    float64\n",
            " 5   Ammonia_ppm                                                 366 non-null    float64\n",
            " 6   Nitrite_ppm                                                 366 non-null    float64\n",
            " 7   Nitrate_ppm                                                 366 non-null    int64  \n",
            " 8   Water_Temp_F                                                366 non-null    float64\n",
            " 9   Room_TempF                                                  366 non-null    float64\n",
            " 10  Dissolved_Oxygen_ppm                                        366 non-null    float64\n",
            " 11  Notes                                                       364 non-null    object \n",
            "dtypes: float64(6), int64(1), object(5)\n",
            "memory usage: 34.4+ KB\n",
            "Preprocessed Aquaponics data saved to /content/preprocessed_aquaponics_data.csv\n",
            "\n",
            "Starting ML model development for Aquaponics data...\n",
            "\n",
            "Training model for: Ammonia_ppm\n",
            "Applying PowerTransformer to target: Ammonia_ppm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-18-117864035.py:126: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.drop(columns=cols_to_drop, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Ammonia_ppm: {'regressor__max_depth': None, 'regressor__min_samples_split': 2, 'regressor__n_estimators': 200}\n",
            "Best cross-validation R2 score for Ammonia_ppm: 0.8100\n",
            "Model for Ammonia_ppm saved to /content/trained_models_aquaponics/aquaponics_Ammonia_ppm.joblib\n",
            "Evaluation on original scale for Ammonia_ppm (after inverse transform):\n",
            "Test Set Mean Squared Error for Ammonia_ppm: 0.1474\n",
            "Test Set R2 Score for Ammonia_ppm: 0.7731\n",
            "\n",
            "Training model for: pH\n",
            "Applying PowerTransformer to target: pH\n",
            "Best parameters for pH: {'regressor__max_depth': 10, 'regressor__min_samples_split': 5, 'regressor__n_estimators': 200}\n",
            "Best cross-validation R2 score for pH: 0.5796\n",
            "Model for pH saved to /content/trained_models_aquaponics/aquaponics_pH.joblib\n",
            "Evaluation on original scale for pH (after inverse transform):\n",
            "Test Set Mean Squared Error for pH: 0.0703\n",
            "Test Set R2 Score for pH: 0.7415\n",
            "\n",
            "Training model for: Dissolved_Oxygen_ppm\n",
            "Applying PowerTransformer to target: Dissolved_Oxygen_ppm\n",
            "Best parameters for Dissolved_Oxygen_ppm: {'regressor__max_depth': 20, 'regressor__min_samples_split': 2, 'regressor__n_estimators': 100}\n",
            "Best cross-validation R2 score for Dissolved_Oxygen_ppm: 0.2593\n",
            "Model for Dissolved_Oxygen_ppm saved to /content/trained_models_aquaponics/aquaponics_Dissolved_Oxygen_ppm.joblib\n",
            "Evaluation on original scale for Dissolved_Oxygen_ppm (after inverse transform):\n",
            "Test Set Mean Squared Error for Dissolved_Oxygen_ppm: 0.8009\n",
            "Test Set R2 Score for Dissolved_Oxygen_ppm: 0.2970\n",
            "\n",
            "Training model for: Water_Temp_F\n",
            "Applying PowerTransformer to target: Water_Temp_F\n",
            "Best parameters for Water_Temp_F: {'regressor__max_depth': 20, 'regressor__min_samples_split': 5, 'regressor__n_estimators': 300}\n",
            "Best cross-validation R2 score for Water_Temp_F: 0.5761\n",
            "Model for Water_Temp_F saved to /content/trained_models_aquaponics/aquaponics_Water_Temp_F.joblib\n",
            "Evaluation on original scale for Water_Temp_F (after inverse transform):\n",
            "Test Set Mean Squared Error for Water_Temp_F: 3.7744\n",
            "Test Set R2 Score for Water_Temp_F: 0.5291\n",
            "\n",
            "Training model for: Nitrate_ppm\n",
            "Applying PowerTransformer to target: Nitrate_ppm\n",
            "Best parameters for Nitrate_ppm: {'regressor__max_depth': None, 'regressor__min_samples_split': 2, 'regressor__n_estimators': 300}\n",
            "Best cross-validation R2 score for Nitrate_ppm: 0.5470\n",
            "Model for Nitrate_ppm saved to /content/trained_models_aquaponics/aquaponics_Nitrate_ppm.joblib\n",
            "Evaluation on original scale for Nitrate_ppm (after inverse transform):\n",
            "Test Set Mean Squared Error for Nitrate_ppm: 563.0968\n",
            "Test Set R2 Score for Nitrate_ppm: 0.5861\n",
            "\n",
            "ML model development for Aquaponics data complete.\n",
            "\n",
            "Aquaponics Evaluation Results:\n",
            "Ammonia_ppm: Test MSE = 0.1474, Test R2 = 0.7731, CV R2 = 0.8100, Best Params = {'regressor__max_depth': None, 'regressor__min_samples_split': 2, 'regressor__n_estimators': 200}\n",
            "pH: Test MSE = 0.0703, Test R2 = 0.7415, CV R2 = 0.5796, Best Params = {'regressor__max_depth': 10, 'regressor__min_samples_split': 5, 'regressor__n_estimators': 200}\n",
            "Dissolved_Oxygen_ppm: Test MSE = 0.8009, Test R2 = 0.2970, CV R2 = 0.2593, Best Params = {'regressor__max_depth': 20, 'regressor__min_samples_split': 2, 'regressor__n_estimators': 100}\n",
            "Water_Temp_F: Test MSE = 3.7744, Test R2 = 0.5291, CV R2 = 0.5761, Best Params = {'regressor__max_depth': 20, 'regressor__min_samples_split': 5, 'regressor__n_estimators': 300}\n",
            "Nitrate_ppm: Test MSE = 563.0968, Test R2 = 0.5861, CV R2 = 0.5470, Best Params = {'regressor__max_depth': None, 'regressor__min_samples_split': 2, 'regressor__n_estimators': 300}\n",
            "\n",
            "Overall process complete.\n"
          ]
        }
      ]
    }
  ]
}